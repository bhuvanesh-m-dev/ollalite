# 🦙 Ollalite – Lightweight Web UI for Local LLMs via Ollama

**Ollalite** is a minimal and efficient Web UI for interacting with local LLMs powered by [Ollama](https://ollama.com). Built for low RAM, CPU, and GPU usage, Ollalite makes switching and running models as smooth as possible on any device.



---

## 🚀 Features

- 🧠 Run LLMs locally with Ollama
- 🔄 Switch between multiple models like `llama3`, `mistral`, `gemma`, etc.
- 🌐 Simple, responsive UI accessible via `localhost`
- 🧩 Lightweight frontend (pure HTML + JS)
- ⚡ Low resource usage – no heavy frameworks
- 📋 Copy-ready code snippets

---
