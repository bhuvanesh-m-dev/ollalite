# ğŸ¦™ Ollalite â€“ Lightweight Web UI for Local LLMs via Ollama

**Ollalite** is a minimal and efficient Web UI for interacting with local LLMs powered by [Ollama](https://ollama.com). Built for low RAM, CPU, and GPU usage, Ollalite makes switching and running models as smooth as possible on any device.



---

## ğŸš€ Features

- ğŸ§  Run LLMs locally with Ollama
- ğŸ”„ Switch between multiple models like `llama3`, `mistral`, `gemma`, etc.
- ğŸŒ Simple, responsive UI accessible via `localhost`
- ğŸ§© Lightweight frontend (pure HTML + JS)
- âš¡ Low resource usage â€“ no heavy frameworks
- ğŸ“‹ Copy-ready code snippets

---
